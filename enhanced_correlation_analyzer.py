#!/usr/bin/env python3
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–≤—Ç–æ—Ä–æ–≤ —Ç–≤–∏—Ç–æ–≤ –∏ –¥–≤–∏–∂–µ–Ω–∏–µ–º —Ä—ã–Ω–∫–∞
"""

import pandas as pd
import asyncio
from datetime import datetime, timedelta
import logging
import re
from dotenv import load_dotenv
from database import get_db_manager, Token, TwitterAuthor, TweetMention
from twitter_profile_parser import TwitterProfileParser
from logger_config import setup_logging

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
setup_logging()
logger = logging.getLogger(__name__)

class EnhancedCorrelationAnalyzer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Ç–≤–∏—Ç–æ–≤ –∏ —Ä—ã–Ω–∫–∞"""
    
    def __init__(self):
        self.db_manager = get_db_manager()
        
    def extract_username_from_tweet(self, tweet_text):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç–≤–∏—Ç–∞"""
        try:
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω @username –≤ —Ç–µ–∫—Å—Ç–µ —Ç–≤–∏—Ç–∞
            pattern = r'@([a-zA-Z0-9_]+)'
            matches = re.findall(pattern, tweet_text)
            
            # –¢–∞–∫–∂–µ –∏—â–µ–º –≤ –Ω–∞—á–∞–ª–µ —Ç–≤–∏—Ç–∞ –ø–æ—Å–ª–µ "RT "
            if tweet_text.startswith('RT @'):
                rt_match = re.match(r'RT @([a-zA-Z0-9_]+)', tweet_text)
                if rt_match:
                    return rt_match.group(1)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if matches:
                return matches[0]
                
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è username: {e}")
            return None
    
    async def analyze_tweets_with_author_metrics(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–≤–∏—Ç—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–≤—Ç–æ—Ä–æ–≤"""
        try:
            session = self.db_manager.Session()
            
            logger.info("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏...")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            tokens = session.query(Token).filter(
                Token.twitter_contract_tweets > 0,
                Token.mint.isnot(None),
                Token.symbol.isnot(None)
            ).order_by(Token.twitter_contract_tweets.desc()).limit(20).all()
            
            logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            
            results = []
            unique_authors = set()
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –±—É–¥—É—Ç –∏–∑ background_monitor)
            simulated_tweets = self.simulate_discovered_tweets(tokens)
            
            async with TwitterProfileParser() as profile_parser:
                for i, tweet_data in enumerate(simulated_tweets, 1):
                    logger.info(f"üì± –ê–Ω–∞–ª–∏–∑ —Ç–≤–∏—Ç–∞ {i}/{len(simulated_tweets)}")
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∞–≤—Ç–æ—Ä–∞
                    author_username = self.extract_username_from_tweet(tweet_data['tweet_text'])
                    if not author_username:
                        continue
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å –∞–≤—Ç–æ—Ä–∞ –µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–ª–∏
                    if author_username not in unique_authors:
                        profile_data = await profile_parser.get_profile(author_username)
                        
                        if profile_data:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≤—Ç–æ—Ä–∞ –≤ –±–∞–∑—É
                            self.db_manager.save_twitter_author(profile_data)
                            unique_authors.add(author_username)
                            
                            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω –ø—Ä–æ—Ñ–∏–ª—å @{author_username}: "
                                       f"{profile_data['followers_count']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–≤–∏—Ç –≤ –±–∞–∑—É
                    tweet_mention_data = {
                        'mint': tweet_data['mint'],
                        'author_username': author_username,
                        'tweet_text': tweet_data['tweet_text'],
                        'tweet_created_at': tweet_data['tweet_created_at'],
                        'discovered_at': datetime.utcnow(),
                        'mention_type': 'contract',
                        'search_query': tweet_data['mint']
                    }
                    
                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –º–æ–º–µ–Ω—Ç —Ç–≤–∏—Ç–∞
                    author = session.query(TwitterAuthor).filter_by(username=author_username).first()
                    if author:
                        tweet_mention_data['author_followers_at_time'] = author.followers_count
                        tweet_mention_data['author_verified_at_time'] = author.is_verified
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ
                    mention = self.db_manager.save_tweet_mention(tweet_mention_data)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    result = {
                        'token': next(t for t in tokens if t.mint == tweet_data['mint']),
                        'tweet_data': tweet_data,
                        'author_data': author.__dict__ if author else None,
                        'mention_id': mention.id if mention else None
                    }
                    results.append(result)
                    
                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    await asyncio.sleep(1)
            
            session.close()
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–≤–∏—Ç–æ–≤: {e}")
            return []
    
    def simulate_discovered_tweets(self, tokens):
        """–°–∏–º—É–ª–∏—Ä—É–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –ø–æ—Å—Ç—É–ø–∞—Ç—å –∏–∑ background_monitor
        simulated_tweets = []
        
        sample_tweet_templates = [
            "üöÄ New gem found! Check out {symbol} at {mint} - this could be huge! #crypto #solana",
            "RT @pumpdotfun: {symbol} just launched! Contract: {mint} üî•",
            "üìà {symbol} is pumping! Contract address: {mint} - get in early!",
            "üíé Found another gem: {symbol} ({mint}) - thank me later üöÄ",
            "üéØ New token alert: {symbol} - {mint} - looks promising!"
        ]
        
        for token in tokens[:10]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 —Ç–æ–∫–µ–Ω–æ–≤
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 1-3 —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
            for i in range(token.twitter_contract_tweets if token.twitter_contract_tweets <= 3 else 1):
                template = sample_tweet_templates[i % len(sample_tweet_templates)]
                tweet_text = template.format(symbol=token.symbol, mint=token.mint)
                
                # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–≤–∏—Ç–∞
                created_time = token.created_at + timedelta(
                    hours=i * 2,  # –¢–≤–∏—Ç—ã —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
                    minutes=i * 15
                )
                
                simulated_tweets.append({
                    'mint': token.mint,
                    'tweet_text': tweet_text,
                    'tweet_created_at': created_time,
                    'token_symbol': token.symbol
                })
        
        return simulated_tweets
    
    def create_correlation_analysis_excel(self, results):
        """–°–æ–∑–¥–∞–µ—Ç Excel —Ñ–∞–π–ª —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"enhanced_correlation_analysis_{timestamp}.xlsx"
            
            # 1. –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å —Ç–≤–∏—Ç–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–≤—Ç–æ—Ä–æ–≤
            tweet_analysis = []
            
            # 2. –°–≤–æ–¥–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º
            author_summary = []
            
            # 3. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–≤—Ç–æ—Ä–æ–≤ –∏ —Ä–µ–∞–∫—Ü–∏–µ–π —Ä—ã–Ω–∫–∞
            correlation_data = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for result in results:
                token = result['token']
                tweet_data = result['tweet_data']
                author_data = result['author_data']
                
                if author_data:
                    # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ç–≤–∏—Ç–æ–≤
                    tweet_row = {
                        '–°–∏–º–≤–æ–ª —Ç–æ–∫–µ–Ω–∞': token.symbol,
                        '–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞': token.name or 'N/A',
                        '–ê–¥—Ä–µ—Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞': token.mint,
                        '–¢–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞': tweet_data['tweet_text'][:200] + '...' if len(tweet_data['tweet_text']) > 200 else tweet_data['tweet_text'],
                        '–ê–≤—Ç–æ—Ä —Ç–≤–∏—Ç–∞': f"@{author_data['username']}",
                        '–û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è': author_data['display_name'] or 'N/A',
                        '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏ –∞–≤—Ç–æ—Ä–∞': author_data['followers_count'],
                        '–¢–≤–∏—Ç—ã –∞–≤—Ç–æ—Ä–∞': author_data['tweets_count'],
                        '–ü–æ–¥–ø–∏—Å–∫–∏ –∞–≤—Ç–æ—Ä–∞': author_data['following_count'],
                        '–õ–∞–π–∫–∏ –∞–≤—Ç–æ—Ä–∞': author_data['likes_count'],
                        '–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω': '–î–∞' if author_data['is_verified'] else '–ù–µ—Ç',
                        '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': author_data['join_date'] or 'N/A',
                        '–ë–∏–æ –∞–≤—Ç–æ—Ä–∞': (author_data['bio'] or '')[:100] + '...' if author_data['bio'] and len(author_data['bio']) > 100 else author_data['bio'] or 'N/A',
                        '–î–∞—Ç–∞ —Ç–≤–∏—Ç–∞': tweet_data['tweet_created_at'].strftime('%Y-%m-%d %H:%M:%S') if tweet_data['tweet_created_at'] else 'N/A',
                        '–í–æ–∑—Ä–∞—Å—Ç —Ç–æ–∫–µ–Ω–∞ –ø—Ä–∏ —Ç–≤–∏—Ç–µ (—á)': f"{(tweet_data['tweet_created_at'] - token.created_at).total_seconds() / 3600:.1f}" if tweet_data['tweet_created_at'] else 'N/A',
                        
                        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–ª–∏—è–Ω–∏—è
                        '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –∞–≤—Ç–æ—Ä–∞': self.categorize_author_influence(author_data),
                        '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ': self.estimate_influence_potential(author_data),
                        
                        # –°—Å—ã–ª–∫–∏
                        '–ü—Ä–æ—Ñ–∏–ª—å –∞–≤—Ç–æ—Ä–∞': f"https://nitter.tiekoetter.com/{author_data['username']}",
                        'DexScreener': f"https://dexscreener.com/solana/{token.mint}",
                        'Pump.fun': f"https://pump.fun/{token.mint}"
                    }
                    tweet_analysis.append(tweet_row)
            
            # –°–≤–æ–¥–∫–∞ –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∞–≤—Ç–æ—Ä–∞–º
            unique_authors = {}
            for result in results:
                author_data = result['author_data']
                if author_data and author_data['username'] not in unique_authors:
                    unique_authors[author_data['username']] = author_data
            
            for username, author_data in unique_authors.items():
                # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ —É–ø–æ–º–∏–Ω–∞–ª —ç—Ç–æ—Ç –∞–≤—Ç–æ—Ä
                mentions_count = sum(1 for r in results if r['author_data'] and r['author_data']['username'] == username)
                
                author_row = {
                    '–ò–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è': f"@{username}",
                    '–û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è': author_data['display_name'] or 'N/A',
                    '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏': author_data['followers_count'],
                    '–¢–≤–∏—Ç—ã': author_data['tweets_count'],
                    '–ü–æ–¥–ø–∏—Å–∫–∏': author_data['following_count'],
                    '–õ–∞–π–∫–∏': author_data['likes_count'],
                    '–í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω': '–î–∞' if author_data['is_verified'] else '–ù–µ—Ç',
                    '–î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏': author_data['join_date'] or 'N/A',
                    '–£–ø–æ–º—è–Ω—É–ª —Ç–æ–∫–µ–Ω–æ–≤': mentions_count,
                    '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –≤–ª–∏—è–Ω–∏—è': self.categorize_author_influence(author_data),
                    '–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ': self.estimate_influence_potential(author_data),
                    'Engagement rate': f"{(author_data['likes_count'] / max(author_data['tweets_count'], 1)):.2f}" if author_data['tweets_count'] > 0 else 'N/A',
                    'Follower/Following ratio': f"{(author_data['followers_count'] / max(author_data['following_count'], 1)):.2f}" if author_data['following_count'] > 0 else 'N/A',
                    '–ü—Ä–æ—Ñ–∏–ª—å': f"https://nitter.tiekoetter.com/{username}"
                }
                author_summary.append(author_row)
            
            # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            if len(results) > 5:
                df_analysis = pd.DataFrame(tweet_analysis)
                
                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø–æ–¥–ø–∏—Å—á–∏–∫–∞–º–∏ –∏ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
                if '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏ –∞–≤—Ç–æ—Ä–∞' in df_analysis.columns:
                    correlation_data.append({
                        '–ú–µ—Ç—Ä–∏–∫–∞ 1': '–ü–æ–¥–ø–∏—Å—á–∏–∫–∏ –∞–≤—Ç–æ—Ä–∞',
                        '–ú–µ—Ç—Ä–∏–∫–∞ 2': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–≤–∏—Ç–æ–≤ –∞–≤—Ç–æ—Ä–∞',
                        '–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è': f"{df_analysis['–ü–æ–¥–ø–∏—Å—á–∏–∫–∏ –∞–≤—Ç–æ—Ä–∞'].corr(df_analysis['–¢–≤–∏—Ç—ã –∞–≤—Ç–æ—Ä–∞']):.4f}",
                        '–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è': '–°–≤—è–∑—å –º–µ–∂–¥—É –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ—Ä–∞ –∏ –µ–≥–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é'
                    })
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø–æ –º–µ—Ä–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –≤–ª–∏—è–Ω–∏–∏ –Ω–∞ —Ä—ã–Ω–æ–∫
            
            # –°–æ–∑–¥–∞–µ–º DataFrame'—ã
            tweet_df = pd.DataFrame(tweet_analysis)
            author_df = pd.DataFrame(author_summary)
            correlation_df = pd.DataFrame(correlation_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                tweet_df.to_excel(writer, sheet_name='–ê–Ω–∞–ª–∏–∑ —Ç–≤–∏—Ç–æ–≤', index=False)
                author_df.to_excel(writer, sheet_name='–°–≤–æ–¥–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º', index=False)
                
                if not correlation_df.empty:
                    correlation_df.to_excel(writer, sheet_name='–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏', index=False)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                stats_data = []
                if not author_df.empty:
                    category_stats = author_df['–ö–∞—Ç–µ–≥–æ—Ä–∏—è –≤–ª–∏—è–Ω–∏—è'].value_counts()
                    for category, count in category_stats.items():
                        stats_data.append({
                            '–ö–∞—Ç–µ–≥–æ—Ä–∏—è': '–í–ª–∏—è–Ω–∏–µ –∞–≤—Ç–æ—Ä–æ–≤',
                            '–ó–Ω–∞—á–µ–Ω–∏–µ': category,
                            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': count,
                            '–ü—Ä–æ—Ü–µ–Ω—Ç': f"{(count/len(author_df)*100):.1f}%"
                        })
                
                if stats_data:
                    stats_df = pd.DataFrame(stats_data)
                    stats_df.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', index=False)
                
                # –ê–≤—Ç–æ–ø–æ–¥–≥–æ–Ω–∫–∞ —à–∏—Ä–∏–Ω—ã –∫–æ–ª–æ–Ω–æ–∫
                for sheet_name in writer.sheets:
                    worksheet = writer.sheets[sheet_name]
                    for column in worksheet.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        adjusted_width = min(max_length + 2, 60)
                        worksheet.column_dimensions[column_letter].width = adjusted_width
                    
                    worksheet.freeze_panes = 'A2'
            
            logger.info(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel: {e}")
            return None
    
    def categorize_author_influence(self, author_data):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤–ª–∏—è–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞"""
        followers = author_data['followers_count']
        is_verified = author_data['is_verified']
        
        if is_verified and followers > 100000:
            return "–ú–∞–∫—Ä–æ-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä"
        elif followers > 50000:
            return "–ú–µ–≥–∞-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä"
        elif followers > 10000:
            return "–ú–∞–∫—Ä–æ-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä"
        elif followers > 1000:
            return "–ú–∏–∫—Ä–æ-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä"
        elif followers > 100:
            return "–ù–∞–Ω–æ-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä"
        else:
            return "–û–±—ã—á–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
    
    def estimate_influence_potential(self, author_data):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–ª–∏—è–Ω–∏—è –∞–≤—Ç–æ—Ä–∞"""
        followers = author_data['followers_count']
        tweets = author_data['tweets_count']
        likes = author_data['likes_count']
        is_verified = author_data['is_verified']
        
        # –ü—Ä–æ—Å—Ç–∞—è —Ñ–æ—Ä–º—É–ª–∞ –æ—Ü–µ–Ω–∫–∏ –≤–ª–∏—è–Ω–∏—è
        base_score = 0
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
        if followers > 100000:
            base_score += 100
        elif followers > 10000:
            base_score += 75
        elif followers > 1000:
            base_score += 50
        elif followers > 100:
            base_score += 25
        
        # –ë–æ–Ω—É—Å –∑–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é
        if is_verified:
            base_score += 25
        
        # –ë–æ–Ω—É—Å –∑–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (engagement)
        if tweets > 0:
            engagement_rate = likes / tweets
            if engagement_rate > 10:
                base_score += 20
            elif engagement_rate > 5:
                base_score += 10
            elif engagement_rate > 1:
                base_score += 5
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
        if base_score >= 100:
            return "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ"
        elif base_score >= 75:
            return "–í—ã—Å–æ–∫–æ–µ"
        elif base_score >= 50:
            return "–°—Ä–µ–¥–Ω–µ–µ"
        elif base_score >= 25:
            return "–ù–∏–∑–∫–æ–µ"
        else:
            return "–û—á–µ–Ω—å –Ω–∏–∑–∫–æ–µ"

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Ç–≤–∏—Ç–æ–≤ –∏ —Ä—ã–Ω–∫–∞")
    
    analyzer = EnhancedCorrelationAnalyzer()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–≤–∏—Ç—ã —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∞–≤—Ç–æ—Ä–æ–≤
    results = await analyzer.analyze_tweets_with_author_metrics()
    
    if not results:
        logger.error("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    logger.info(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results)} —Ç–≤–∏—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç
    filename = analyzer.create_correlation_analysis_excel(results)
    
    if filename:
        logger.info(f"üìà –£–õ–£–ß–®–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–û–†–†–ï–õ–Ø–¶–ò–ô:")
        logger.info(f"  ‚Ä¢ –§–∞–π–ª: {filename}")
        logger.info(f"  ‚Ä¢ –¢–≤–∏—Ç–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(results)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–≤—Ç–æ—Ä–∞–º
        unique_authors = set()
        macro_influencers = 0
        verified_authors = 0
        
        for result in results:
            author_data = result['author_data']
            if author_data:
                username = author_data['username']
                if username not in unique_authors:
                    unique_authors.add(username)
                    
                    if author_data['followers_count'] > 10000:
                        macro_influencers += 1
                    
                    if author_data['is_verified']:
                        verified_authors += 1
        
        logger.info(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {len(unique_authors)}")
        logger.info(f"  ‚Ä¢ –ú–∞–∫—Ä–æ-–∏–Ω—Ñ–ª—é–µ–Ω—Å–µ—Ä–æ–≤ (>10k): {macro_influencers}")
        logger.info(f"  ‚Ä¢ –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≤—Ç–æ—Ä–æ–≤: {verified_authors}")
    
    logger.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    asyncio.run(main())
 