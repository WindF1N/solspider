#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –ø—Ä–æ—Ñ–∏–ª–µ–π –∞–≤—Ç–æ—Ä–æ–≤ —Ç–≤–∏—Ç–æ–≤ —Å Nitter
"""

import re
import aiohttp
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from dynamic_cookie_rotation import get_background_proxy_cookie_async
from anubis_handler import handle_anubis_challenge_for_session

logger = logging.getLogger(__name__)

class TwitterProfileParser:
    """–ü–∞—Ä—Å–µ—Ä –ø—Ä–æ—Ñ–∏–ª–µ–π –∞–≤—Ç–æ—Ä–æ–≤ —Ç–≤–∏—Ç–æ–≤"""
    
    def __init__(self):
        self.session = None
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä—ã–π cookie_rotator - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É —Å Anubis handler
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def parse_number(self, text):
        """–ü–∞—Ä—Å–∏—Ç —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç K, M —Ñ–æ—Ä–º–∞—Ç—ã)"""
        if not text:
            return 0
            
        text = text.replace(',', '').strip()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\d.KMBkmb]', '', text)
        
        if not text:
            return 0
        
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
            if text.upper().endswith('K'):
                return int(float(text[:-1]) * 1000)
            elif text.upper().endswith('M'):
                return int(float(text[:-1]) * 1000000)
            elif text.upper().endswith('B'):
                return int(float(text[:-1]) * 1000000000)
            else:
                return int(float(text))
        except (ValueError, IndexError):
            return 0
    
    def extract_clean_text(self, element):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏"""
        try:
            # –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
            element_copy = element.__copy__()
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–∞–º–∏ –∏ –¥—Ä—É–≥–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            for link in element_copy.find_all('a'):
                if link.string:
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    if link.previous_sibling and not str(link.previous_sibling).endswith(' '):
                        link.insert_before(' ')
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ —Å—Å—ã–ª–∫–∏ –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç  
                    if link.next_sibling and not str(link.next_sibling).startswith(' '):
                        link.insert_after(' ')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –¥—Ä—É–≥–∏–º–∏ –≤–∞–∂–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
            for elem in element_copy.find_all(['span', 'div']):
                if elem.get_text(strip=True) and elem.parent == element_copy:
                    if elem.previous_sibling and not str(elem.previous_sibling).endswith(' '):
                        elem.insert_before(' ')
                    if elem.next_sibling and not str(elem.next_sibling).startswith(' '):
                        elem.insert_after(' ')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
            text = element_copy.get_text(separator=' ', strip=True)
            
            # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ
            text = re.sub(r'\s+', ' ', text)
            
            return text.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {e}")
            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –º–µ—Ç–æ–¥—É
            return element.get_text(strip=True)
    
    def extract_contracts_from_text(self, text):
        """
        –ï–î–ò–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è Solana –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏ Ethereum –∞–¥—Ä–µ—Å–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Ç–≤–∏—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏ –∞–¥—Ä–µ—Å–æ–≤
        """
        if not text:
            return []
        
        all_contracts = []
        
        # 1. –ò—â–µ–º Ethereum –∞–¥—Ä–µ—Å–∞ (0x + 40 hex —Å–∏–º–≤–æ–ª–æ–≤)
        eth_addresses = re.findall(r'\b0x[A-Fa-f0-9]{40}\b', text)
        all_contracts.extend(eth_addresses)
        
        # 2. –ò—â–µ–º Solana –∞–¥—Ä–µ—Å–∞ (32-44 —Å–∏–º–≤–æ–ª–∞, –±—É–∫–≤—ã –∏ —Ü–∏—Ñ—Ä—ã)
        solana_contracts = re.findall(r'\b[A-Za-z0-9]{32,44}\b', text)
        all_contracts.extend(solana_contracts)
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
        clean_contracts = []
        for contract in all_contracts:
            # –£–±–∏—Ä–∞–µ–º "pump" —Å –∫–æ–Ω—Ü–∞ –µ—Å–ª–∏ –µ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è Solana)
            clean_contract = contract
            if contract.endswith('pump') and not contract.startswith('0x'):
                clean_contract = contract[:-4]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∞–¥—Ä–µ—Å–∞
            is_eth_address = clean_contract.startswith('0x') and len(clean_contract) == 42 and re.match(r'0x[A-Fa-f0-9]{40}', clean_contract)
            is_solana_address = 32 <= len(clean_contract) <= 44 and clean_contract.isalnum() and not clean_contract.startswith('0x')
            
            if is_eth_address:
                # Ethereum –∞–¥—Ä–µ—Å - –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –Ω–µ –Ω—É–ª–µ–≤–æ–π
                if not clean_contract.lower() in ['0x0000000000000000000000000000000000000000']:
                    clean_contracts.append(clean_contract)
            elif is_solana_address:
                # Solana –∞–¥—Ä–µ—Å - –ø—Ä–∏–º–µ–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–≥–∏–∫—É
                clean_contracts.append(clean_contract)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
        return list(set(clean_contracts))
    
    def extract_profile_data(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è
            profile_data = {
                'username': None,
                'display_name': None,
                'bio': None,
                'website': None,
                'join_date': None,
                'is_verified': False,
                'avatar_url': None,
                'tweets_count': 0,
                'following_count': 0,
                'followers_count': 0,
                'likes_count': 0
            }
            
            # –ò—â–µ–º –±–ª–æ–∫ –ø—Ä–æ—Ñ–∏–ª—è
            profile_card = soup.find('div', class_='profile-card')
            if not profile_card:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É Nitter
                if "Making sure you're not a bot!" in html_content:
                    logger.warning("‚ö†Ô∏è Nitter –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω - —Ç—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ cookies")
                else:
                    logger.warning("‚ö†Ô∏è –ë–ª–æ–∫ profile-card –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
            username_elem = profile_card.find('a', class_='profile-card-username')
            if username_elem:
                username_text = username_elem.get_text(strip=True)
                if username_text:
                    profile_data['username'] = username_text.replace('@', '')
                else:
                    logger.warning("‚ö†Ô∏è –ü—É—Å—Ç–æ–µ –∏–º—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
            
            fullname_elem = profile_card.find('a', class_='profile-card-fullname')
            if fullname_elem:
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –±–µ–∑ –∏–∫–æ–Ω–æ–∫ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
                display_name = fullname_elem.get_text(strip=True)
                if display_name:
                    profile_data['display_name'] = display_name
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é
                verified_icon = fullname_elem.find('span', class_='verified-icon')
                profile_data['is_verified'] = verified_icon is not None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤–∞—Ç–∞—Ä
            avatar_elem = profile_card.find('img')
            if avatar_elem and avatar_elem.get('src'):
                profile_data['avatar_url'] = avatar_elem['src']
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–æ
            bio_elem = profile_card.find('div', class_='profile-bio')
            if bio_elem:
                profile_data['bio'] = bio_elem.get_text(strip=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–±-—Å–∞–π—Ç
            website_elem = profile_card.find('div', class_='profile-website')
            if website_elem:
                website_link = website_elem.find('a')
                if website_link:
                    profile_data['website'] = website_link.get('href') or website_link.get_text(strip=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
            joindate_elem = profile_card.find('div', class_='profile-joindate')
            if joindate_elem:
                profile_data['join_date'] = joindate_elem.get_text(strip=True).replace('Joined ', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stat_list = profile_card.find('ul', class_='profile-statlist')
            if stat_list:
                stats = stat_list.find_all('li')
                
                for stat in stats:
                    header = stat.find('span', class_='profile-stat-header')
                    number = stat.find('span', class_='profile-stat-num')
                    
                    if header and number:
                        header_text = header.get_text(strip=True).lower()
                        number_text = number.get_text(strip=True)
                        parsed_number = self.parse_number(number_text)
                        
                        if 'tweets' in header_text or 'posts' in header_text:
                            profile_data['tweets_count'] = parsed_number
                        elif 'following' in header_text:
                            profile_data['following_count'] = parsed_number
                        elif 'followers' in header_text:
                            profile_data['followers_count'] = parsed_number
                        elif 'likes' in header_text:
                            profile_data['likes_count'] = parsed_number
            
            username = profile_data.get('username', 'Unknown')
            logger.info(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}: "
                       f"{profile_data['followers_count']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤, "
                       f"{profile_data['tweets_count']} —Ç–≤–∏—Ç–æ–≤")
            
            return profile_data
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–æ—Ñ–∏–ª—è: {e}")
            return None
    
    async def get_profile(self, username):
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∫—É–∫–∏ –∏ Anubis handler"""
        try:
            # –£–±–∏—Ä–∞–µ–º @ –µ—Å–ª–∏ –µ—Å—Ç—å
            username = username.replace('@', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ session –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                logger.error(f"‚ùå Session –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è @{username}")
                return None
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ cookies —á–µ—Ä–µ–∑ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
            proxy, cookies_string = await get_background_proxy_cookie_async(self.session)
            
            # –î–ª—è IP-–∞–¥—Ä–µ—Å–æ–≤ Nitter cookies –Ω–µ –Ω—É–∂–Ω—ã (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
            if cookies_string is None:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è @{username}")
                return None
                
            logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å @{username} (–ø—Ä–æ–∫—Å–∏: {'‚úÖ' if proxy else '‚ùå'})")
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É cookies –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è aiohttp
            cookies = {}
            try:
                for cookie_part in cookies_string.split(';'):
                    if '=' in cookie_part:
                        key, value = cookie_part.strip().split('=', 1)
                        cookies[key] = value
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cookies –¥–ª—è @{username}: {e}")
                return None
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–æ–º–µ–Ω–∞
            try:
                from duplicate_groups_manager import get_nitter_domain_and_url, add_host_header_if_needed
                current_domain, nitter_base = get_nitter_domain_and_url()
            except ImportError:
                current_domain = "185.207.1.206:8085"
                nitter_base = "http://185.207.1.206:8085"
            url = f"{nitter_base}/{username}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ Host –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö IP-–∞–¥—Ä–µ—Å–æ–≤
            add_host_header_if_needed(headers, current_domain)
            
            async with self.session.get(url, headers=headers, cookies=cookies) as response:
                html_content = await response.text()
                
                # üîç –ü–†–û–í–ï–†–Ø–ï–ú –ù–ê ANUBIS CHALLENGE
                if ('id="anubis_challenge"' in html_content or "Making sure you're not a bot!" in html_content):
                    logger.warning(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω Anubis challenge –¥–ª—è @{username} - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞–µ–º...")
                    
                    try:
                        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞–µ–º challenge
                        new_cookies = await handle_anubis_challenge_for_session(
                            self.session, url, html_content, force_fresh_challenge=True
                        )
                        
                        if new_cookies:
                            logger.info(f"‚úÖ Challenge —Ä–µ—à–µ–Ω –¥–ª—è @{username}, –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å...")
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º cookies –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                            for key, value in new_cookies.items():
                                cookies[key] = value
                                
                            async with self.session.get(url, headers=headers, cookies=cookies) as retry_response:
                                if retry_response.status == 200:
                                    html_content = await retry_response.text()
                                    profile_data = self.extract_profile_data(html_content)
                                    
                                    if profile_data and profile_data.get('username'):
                                        logger.info(f"‚úÖ –ü—Ä–æ—Ñ–∏–ª—å @{username} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –ø–æ—Å–ª–µ —Ä–µ—à–µ–Ω–∏—è challenge")
                                        return profile_data
                                    else:
                                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è @{username} –ø–æ—Å–ª–µ challenge")
                                        return None
                                else:
                                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ challenge @{username}: {retry_response.status}")
                                    return None
                        else:
                            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–µ—à–∏—Ç—å challenge –¥–ª—è @{username}")
                            return None
                            
                    except Exception as challenge_error:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ—à–µ–Ω–∏—è challenge –¥–ª—è @{username}: {challenge_error}")
                        return None
                
                elif response.status == 200:
                    profile_data = self.extract_profile_data(html_content)
                    
                    if profile_data and profile_data.get('username'):
                        return profile_data
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                        return None
                        
                elif response.status == 429:
                    logger.warning(f"‚ö†Ô∏è Rate limit –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                    
                    # –ü—Ä–∏ 429 –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–º–µ–Ω Nitter, –∞ –Ω–µ –º–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏
                    from nitter_domain_rotator import get_next_nitter_domain
                    new_domain = get_next_nitter_domain()
                    logger.warning(f"üåê HTTP 429 - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {new_domain}")
                    
                    await asyncio.sleep(2)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                    
                    # –ù–ò–ö–û–ì–î–ê –ù–ï –°–î–ê–ï–ú–°–Ø! –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º —Å–µ–±—è —Å –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–æ–º
                    return await self.get_profile(username)
                    
                elif response.status == 404:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ—Ñ–∏–ª—å @{username} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return None
                    
                else:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è @{username}: {response.status}")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è @{username}: {e}")
            return None
    
    def extract_tweets_from_profile(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–≤–∏—Ç—ã —Å –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∏—Å–∫–ª—é—á–∞—è —Ä–µ—Ç–≤–∏—Ç—ã)"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            tweets = []
            retweets_skipped = 0
            
            # –ò—â–µ–º –≤—Å–µ —Ç–≤–∏—Ç—ã –≤ timeline
            timeline_items = soup.find_all('div', class_='timeline-item')
            
            for item in timeline_items:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ retweet-header - –µ—Å–ª–∏ –µ—Å—Ç—å, —Ç–æ —ç—Ç–æ —Ä–µ—Ç–≤–∏—Ç
                retweet_header = item.find('div', class_='retweet-header')
                if retweet_header:
                    retweets_skipped += 1
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ—Ç–≤–∏—Ç—ã
                
                tweet_content = item.find('div', class_='tweet-content')
                if tweet_content:
                    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                    tweet_text = self.extract_clean_text(tweet_content)
                    if tweet_text:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É —Ç–≤–∏—Ç–∞
                        tweet_date_elem = item.find('span', class_='tweet-date')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL —Ç–≤–∏—Ç–∞
                        tweet_link = item.find('a', class_='tweet-link')
                        tweet_url = tweet_link.get('href', '') if tweet_link else ''
                        
                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å duplicate_groups_manager)
                        tweet_dict = {
                            'text': tweet_text,
                            'date': tweet_date_elem,  # –≠–ª–µ–º–µ–Ω—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ duplicate_groups_manager
                            'url': tweet_url
                        }
                        tweets.append(tweet_dict)
            
            logger.info(f"üì± –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(tweets)} –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ —Å –ø—Ä–æ—Ñ–∏–ª—è (–ø—Ä–æ–ø—É—â–µ–Ω–æ {retweets_skipped} —Ä–µ—Ç–≤–∏—Ç–æ–≤)")
            return tweets
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–≤–∏—Ç–æ–≤: {e}")
            return []
    
    def extract_next_page_url(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç show-more —Å —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            show_more = soup.find('div', class_='show-more')
            if show_more:
                link = show_more.find('a')
                if link and link.get('href'):
                    next_url = link.get('href')
                    logger.debug(f"üîó –ù–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {next_url}")
                    return next_url
            
            logger.debug("üìÑ –°—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}")
            return None
    
    def extract_tweets_with_contracts(self, html_content):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–≤–∏—Ç—ã —Å –ø—Ä–æ—Ñ–∏–ª—è –∏ –∏—â–µ—Ç –≤ –Ω–∏—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã (–∞–¥—Ä–µ—Å–∞ –¥–ª–∏–Ω–æ–π 32-44 —Å–∏–º–≤–æ–ª–∞), –∏—Å–∫–ª—é—á–∞—è —Ä–µ—Ç–≤–∏—Ç—ã"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            tweets_with_contracts = []
            retweets_skipped = 0
            
            # –ò—â–µ–º –≤—Å–µ —Ç–≤–∏—Ç—ã –≤ timeline
            timeline_items = soup.find_all('div', class_='timeline-item')
            
            for item in timeline_items:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ retweet-header - –µ—Å–ª–∏ –µ—Å—Ç—å, —Ç–æ —ç—Ç–æ —Ä–µ—Ç–≤–∏—Ç
                retweet_header = item.find('div', class_='retweet-header')
                if retweet_header:
                    retweets_skipped += 1
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–µ—Ç–≤–∏—Ç—ã
                
                tweet_content = item.find('div', class_='tweet-content')
                if tweet_content:
                    # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
                    tweet_text = self.extract_clean_text(tweet_content)
                    if tweet_text:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
                        contracts = self.extract_contracts_from_text(tweet_text)
                        
                        if contracts:
                            # –¢–∞–∫–∂–µ –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É —Ç–≤–∏—Ç–∞
                            tweet_date = None
                            date_element = item.find('span', class_='tweet-date')
                            if date_element:
                                date_link = date_element.find('a')
                                if date_link:
                                    tweet_date = date_link.get('title', date_link.get_text(strip=True))
                            
                            tweets_with_contracts.append({
                                'text': tweet_text,
                                'contracts': contracts,
                                'date': tweet_date
                            })
            
            logger.info(f"üì± –ù–∞–π–¥–µ–Ω–æ {len(tweets_with_contracts)} –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏ (–ø—Ä–æ–ø—É—â–µ–Ω–æ {retweets_skipped} —Ä–µ—Ç–≤–∏—Ç–æ–≤)")
            return tweets_with_contracts
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–≤–∏—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏: {e}")
            return []
    
    async def get_profile_with_replies_multi_page(self, username, max_pages=3):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–º–µ—Å—Ç–µ —Å —Ç–≤–∏—Ç–∞–º–∏ –∏–∑ /with_replies 
        —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –¥–æ max_pages —Å—Ç—Ä–∞–Ω–∏—Ü
        """
        try:
            # –£–±–∏—Ä–∞–µ–º @ –µ—Å–ª–∏ –µ—Å—Ç—å
            username = username.replace('@', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ session –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                logger.error(f"‚ùå Session –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è @{username}")
                return None, []
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ cookies —á–µ—Ä–µ–∑ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
            proxy, cookies_string = await get_background_proxy_cookie_async(self.session)
            
            # –î–ª—è IP-–∞–¥—Ä–µ—Å–æ–≤ Nitter cookies –Ω–µ –Ω—É–∂–Ω—ã (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
            if cookies_string is None:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è @{username}")
                return None, []
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É cookies –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è aiohttp
            cookies = {}
            try:
                for cookie_part in cookies_string.split(';'):
                    if '=' in cookie_part:
                        key, value = cookie_part.strip().split('=', 1)
                        cookies[key] = value
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cookies –¥–ª—è @{username}: {e}")
                return None, []
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–æ–º–µ–Ω–∞
            try:
                from duplicate_groups_manager import get_nitter_domain_and_url, add_host_header_if_needed
                current_domain, nitter_base = get_nitter_domain_and_url()
            except ImportError:
                # Fallback –Ω–∞ IP-–∞–¥—Ä–µ—Å –µ—Å–ª–∏ –Ω–µ —É–¥–∞–µ—Ç—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å
                current_domain = "185.207.1.206:8085"
                nitter_base = "http://185.207.1.206:8085"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ Host –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö IP-–∞–¥—Ä–µ—Å–æ–≤
            add_host_header_if_needed(headers, current_domain)
            
            # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ—Ñ–∏–ª—è —Å /with_replies
            base_url = f"{nitter_base}/{username}/with_replies"
            current_url = base_url
            
            profile_data = None
            all_tweets = []
            all_tweets_with_contracts = []
            page_count = 0
            
            logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å @{username} —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π replies (–¥–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ session –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                logger.error(f"‚ùå Session –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è @{username}")
                return None, []
            
            while page_count < max_pages and current_url:
                page_count += 1
                logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_count}/{max_pages} –¥–ª—è @{username}")
                
                try:
                    async with self.session.get(current_url, headers=headers, cookies=cookies, timeout=15) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            
                            # üîç –ü–†–û–í–ï–†–Ø–ï–ú –ù–ê ANUBIS CHALLENGE
                            if ('id="anubis_challenge"' in html_content or "Making sure you're not a bot!" in html_content):
                                logger.warning(f"üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω Anubis challenge –¥–ª—è @{username} - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞–µ–º...")
                                
                                try:
                                    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ—à–∞–µ–º challenge
                                    new_cookies = await handle_anubis_challenge_for_session(
                                        self.session, current_url, html_content, force_fresh_challenge=True
                                    )
                                    
                                    if new_cookies:
                                        logger.info(f"‚úÖ Challenge —Ä–µ—à–µ–Ω –¥–ª—è @{username}, –æ–±–Ω–æ–≤–ª—è–µ–º cookies...")
                                        # –û–±–Ω–æ–≤–ª—è–µ–º cookies –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                                        for key, value in new_cookies.items():
                                            cookies[key] = value
                                        # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å —Å –Ω–æ–≤—ã–º–∏ cookies
                                        continue
                                    else:
                                        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–µ—à–∏—Ç—å challenge –¥–ª—è @{username}")
                                        break
                                        
                                except Exception as challenge_error:
                                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ—à–µ–Ω–∏—è challenge –¥–ª—è @{username}: {challenge_error}")
                                    break
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è —Ç–æ–ª—å–∫–æ —Å –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            if page_count == 1:
                                profile_data = self.extract_profile_data(html_content)
                                if not profile_data:
                                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                                    break
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–≤–∏—Ç—ã —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            page_tweets = self.extract_tweets_from_profile(html_content)
                            page_tweets_with_contracts = self.extract_tweets_with_contracts(html_content)
                            
                            all_tweets.extend(page_tweets)
                            all_tweets_with_contracts.extend(page_tweets_with_contracts)
                            
                            logger.info(f"üì± –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_count}: –Ω–∞–π–¥–µ–Ω–æ {len(page_tweets)} —Ç–≤–∏—Ç–æ–≤, {len(page_tweets_with_contracts)} —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏")
                            
                            # –ò—â–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                            next_page_path = self.extract_next_page_url(html_content)
                            if next_page_path and page_count < max_pages:
                                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                                if next_page_path.startswith('?'):
                                    current_url = f"{base_url}{next_page_path}"
                                else:
                                    current_url = f"{nitter_base}{next_page_path}"
                                
                                logger.debug(f"üîó –°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {current_url}")
                                
                                # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                                await asyncio.sleep(3)
                            else:
                                logger.info(f"üìÑ –ë–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ—Ç –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –¥–ª—è @{username}")
                                break
                                
                        elif response.status == 429:
                            logger.warning(f"‚ö†Ô∏è Rate limit –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count} –¥–ª—è @{username}")
                            
                            # –ü—Ä–∏ 429 –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–º–µ–Ω Nitter, –∞ –Ω–µ –º–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏
                            from nitter_domain_rotator import get_next_nitter_domain
                            new_domain = get_next_nitter_domain()
                            logger.warning(f"üåê HTTP 429 - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {new_domain}")
                            
                            # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–π URL —Å –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–æ–º
                            from urllib.parse import urlparse
                            parsed_url = urlparse(current_url)
                            new_base_url = f"http://{new_domain}" if new_domain.count('.') >= 3 else f"https://{new_domain}"
                            current_url = f"{new_base_url}{parsed_url.path}"
                            if parsed_url.query:
                                current_url += f"?{parsed_url.query}"
                                
                            await asyncio.sleep(2)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                            page_count -= 1  # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç—É –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–æ–º
                            continue
                            
                        elif response.status == 404:
                            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ—Ñ–∏–ª—å @{username} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                            break
                            
                        else:
                            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count} –¥–ª—è @{username}: {response.status}")
                            break
                            
                except asyncio.TimeoutError:
                    logger.warning(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count} –¥–ª—è @{username}")
                    break
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_count} –¥–ª—è @{username}: {e}")
                    break
            
            # –ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
            total_tweets = len(all_tweets)
            total_contracts = len(all_tweets_with_contracts)
            
            if profile_data:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–∞—Ö –≤ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è
                profile_data['total_loaded_tweets'] = total_tweets
                profile_data['tweets_with_contracts'] = total_contracts
                profile_data['pages_loaded'] = page_count
                
                logger.info(f"‚úÖ @{username}: –ø—Ä–æ—Ñ–∏–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω, {page_count} —Å—Ç—Ä–∞–Ω–∏—Ü, {total_tweets} —Ç–≤–∏—Ç–æ–≤, {total_contracts} —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏")
                return profile_data, all_tweets, all_tweets_with_contracts
            else:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å @{username}")
                return None, all_tweets, all_tweets_with_contracts
                
        except Exception as e:
            logger.error(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è —Å replies @{username}: {e}")
            return None, [], []

    async def get_profile_with_tweets(self, username, retry_count=0, max_retries=3):
        """–ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤–º–µ—Å—Ç–µ —Å —Ç–≤–∏—Ç–∞–º–∏ —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫"""
        try:
            # –£–±–∏—Ä–∞–µ–º @ –µ—Å–ª–∏ –µ—Å—Ç—å
            username = username.replace('@', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ session –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                logger.error(f"‚ùå Session –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è @{username}")
                return None, []
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ cookies —á–µ—Ä–µ–∑ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É
            proxy, cookies_string = await get_background_proxy_cookie_async(self.session)
            
            # –î–ª—è IP-–∞–¥—Ä–µ—Å–æ–≤ Nitter cookies –Ω–µ –Ω—É–∂–Ω—ã (–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)
            if cookies_string is None:
                logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å cookies –¥–ª—è @{username}")
                return None, []
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É cookies –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è aiohttp
            cookies = {}
            try:
                for cookie_part in cookies_string.split(';'):
                    if '=' in cookie_part:
                        key, value = cookie_part.strip().split('=', 1)
                        cookies[key] = value
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cookies –¥–ª—è @{username}: {e}")
                return None, []
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –¥–æ–º–µ–Ω–∞
            try:
                from duplicate_groups_manager import get_nitter_domain_and_url, add_host_header_if_needed
                current_domain, nitter_base = get_nitter_domain_and_url()
            except ImportError:
                current_domain = "185.207.1.206:8085"
                nitter_base = "http://185.207.1.206:8085"
            url = f"{nitter_base}/{username}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ Host –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö IP-–∞–¥—Ä–µ—Å–æ–≤
            add_host_header_if_needed(headers, current_domain)
            
            if retry_count == 0:
                logger.info(f"üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å —Å —Ç–≤–∏—Ç–∞–º–∏ @{username}")
            else:
                logger.warning(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {retry_count}/{max_retries} –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ session –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
            if not self.session:
                logger.error(f"‚ùå Session –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è @{username}")
                return None, []
            
            async with self.session.get(url, headers=headers, cookies=cookies) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è
                    profile_data = self.extract_profile_data(html_content)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–≤–∏—Ç—ã
                    tweets = self.extract_tweets_from_profile(html_content)
                    
                    if profile_data and profile_data.get('username'):
                        return profile_data, tweets
                    else:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                        return None, tweets
                        
                elif response.status == 429:
                    logger.warning(f"‚ö†Ô∏è Rate limit –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                    
                    # –ü—Ä–∏ 429 –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–º–µ–Ω Nitter, –∞ –Ω–µ –º–µ–Ω—è–µ–º –ø—Ä–æ–∫—Å–∏
                    from nitter_domain_rotator import get_next_nitter_domain
                    new_domain = get_next_nitter_domain()
                    logger.warning(f"üåê HTTP 429 - –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –Ω–æ–≤—ã–π –¥–æ–º–µ–Ω: {new_domain}")
                    
                    await asyncio.sleep(2)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                    
                    # –ù–ò–ö–û–ì–î–ê –ù–ï –°–î–ê–ï–ú–°–Ø! –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ–º —Å–µ–±—è —Å –Ω–æ–≤—ã–º –¥–æ–º–µ–Ω–æ–º
                    return await self.get_profile_with_tweets(username, retry_count, max_retries)
                    
                elif response.status == 404:
                    logger.warning(f"‚ö†Ô∏è –ü—Ä–æ—Ñ–∏–ª—å @{username} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return None, []
                    
                else:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è @{username}: {response.status}")
                    return None, []
                    
        except Exception as e:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏
            is_network_error = (
                "Cannot connect to host" in str(e) or
                "Network is unreachable" in str(e) or
                "Connection timeout" in str(e) or
                "TimeoutError" in str(e) or
                "ClientConnectorError" in str(e)
            )
            
            if is_network_error and retry_count < max_retries:
                retry_delay = 2 + retry_count  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                logger.warning(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è @{username}: {e}")
                logger.info(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —á–µ—Ä–µ–∑ {retry_delay}—Å (–ø–æ–ø—ã—Ç–∫–∞ {retry_count + 1}/{max_retries})")
                await asyncio.sleep(retry_delay)
                return await self.get_profile_with_tweets(username, retry_count + 1, max_retries)
            else:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è —Å —Ç–≤–∏—Ç–∞–º–∏ @{username}: {e}")
                return None, []

    async def get_multiple_profiles(self, usernames, delay=1.0):
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        profiles = {}
        
        for i, username in enumerate(usernames, 1):
            logger.info(f"üìà –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ñ–∏–ª—è {i}/{len(usernames)}: @{username}")
            
            try:
                profile_data = await self.get_profile(username)
                if profile_data and profile_data.get('username'):
                    profiles[username] = profile_data
                else:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ—Ñ–∏–ª—è @{username}")
                    profiles[username] = None
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è @{username}: {e}")
                profiles[username] = None
            
            # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –ø—Ä–æ—Ñ–∏–ª–µ–π
            if i < len(usernames):
                await asyncio.sleep(max(delay, 3.0))
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len([p for p in profiles.values() if p])} –∏–∑ {len(usernames)} –ø—Ä–æ—Ñ–∏–ª–µ–π")
        return profiles

    async def analyze_author_contracts_advanced(self, username, max_pages=3):
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã —É –∞–≤—Ç–æ—Ä–∞ Twitter —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π /with_replies –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞—Ö
        """
        try:
            profile_data, all_tweets, tweets_with_contracts = await self.get_profile_with_replies_multi_page(username, max_pages)
            
            if not profile_data:
                return {
                    'success': False,
                    'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å',
                    'total_tweets': 0,
                    'tweets_with_contracts': 0,
                    'unique_contracts': 0,
                    'contracts_analysis': {}
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã
            all_contracts = []
            contract_frequency = {}
            
            for tweet in tweets_with_contracts:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ tweet —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
                if not isinstance(tweet, dict):
                    logger.debug(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ç–≤–∏—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–∏–ø–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤: {type(tweet)}")
                    continue
                    
                contracts = tweet.get('contracts', [])
                if not isinstance(contracts, list):
                    logger.debug(f"‚ö†Ô∏è –ü–æ–ª–µ contracts –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º: {type(contracts)}")
                    continue
                    
                for contract in contracts:
                    if isinstance(contract, str) and 32 <= len(contract) <= 44 and contract.isalnum():
                        all_contracts.append(contract)
                        contract_frequency[contract] = contract_frequency.get(contract, 0) + 1
            
            unique_contracts = len(set(all_contracts))
            total_contract_mentions = len(all_contracts)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if len(tweets_with_contracts) > 0:
                contract_diversity_percent = (unique_contracts / len(tweets_with_contracts)) * 100
                
                # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–µ–º—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç
                if contract_frequency:
                    most_frequent_contract = max(contract_frequency.items(), key=lambda x: x[1])
                    max_contract_concentration = (most_frequent_contract[1] / total_contract_mentions) * 100
                else:
                    most_frequent_contract = None
                    max_contract_concentration = 0
            else:
                contract_diversity_percent = 0
                most_frequent_contract = None
                max_contract_concentration = 0
            
            # –¢–æ–ø-5 —Å–∞–º—ã—Ö —É–ø–æ–º–∏–Ω–∞–µ–º—ã—Ö –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤
            top_contracts = sorted(contract_frequency.items(), key=lambda x: x[1], reverse=True)[:5]
            
            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
            total_tweets_analyzed = len(all_tweets)
            
            if unique_contracts == 0:
                quality_analysis = "–ù–µ—Ç –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤"
                spam_likelihood = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            elif max_contract_concentration >= 80:
                quality_analysis = "–û—Ç–ª–∏—á–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è"
                spam_likelihood = "–ù–∏–∑–∫–∞—è"
            elif max_contract_concentration >= 60:
                quality_analysis = "–•–æ—Ä–æ—à–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è"
                spam_likelihood = "–ù–∏–∑–∫–∞—è"
            elif max_contract_concentration >= 40:
                quality_analysis = "–£–º–µ—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è"
                spam_likelihood = "–°—Ä–µ–¥–Ω—è—è"
            else:
                # –ê–î–ê–ü–¢–ò–í–ù–´–ï –ü–û–†–û–ì–ò –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–≤–∏—Ç–æ–≤
                diversity_threshold = 40  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫
                
                if total_tweets_analyzed < 10:
                    diversity_threshold = 50  # –ú—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫
                elif total_tweets_analyzed < 20:
                    diversity_threshold = 30  # –£–º–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –≤—ã–±–æ—Ä–æ–∫
                else:
                    diversity_threshold = 40  # –£–º–µ—Ä–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –≤—ã–±–æ—Ä–æ–∫
                
                if contract_diversity_percent >= diversity_threshold:
                    quality_analysis = f"–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (>{diversity_threshold}% –¥–ª—è {total_tweets_analyzed} —Ç–≤–∏—Ç–æ–≤)"
                    spam_likelihood = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
                else:
                    quality_analysis = f"–ü—Ä–∏–µ–º–ª–µ–º–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (<{diversity_threshold}% –¥–ª—è {total_tweets_analyzed} —Ç–≤–∏—Ç–æ–≤)"
                    spam_likelihood = "–ù–∏–∑–∫–∞—è"
            
            result = {
                'success': True,
                'profile_data': profile_data,
                'total_tweets_loaded': len(all_tweets),
                'tweets_with_contracts': len(tweets_with_contracts),
                'unique_contracts': unique_contracts,
                'total_contract_mentions': total_contract_mentions,
                'contract_diversity_percent': round(contract_diversity_percent, 1),
                'max_contract_concentration_percent': round(max_contract_concentration, 1),
                'most_frequent_contract': most_frequent_contract,
                'top_contracts': top_contracts,
                'quality_analysis': quality_analysis,
                'spam_likelihood': spam_likelihood,
                'pages_analyzed': profile_data.get('pages_loaded', 0),
                'contracts_details': tweets_with_contracts
            }
            
            logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ @{username}: {unique_contracts} —É–Ω–∏–∫. –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∏–∑ {total_contract_mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, {quality_analysis}")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–æ–≤ –∞–≤—Ç–æ—Ä–∞ @{username}: {e}")
            return {
                'success': False,
                'error': str(e),
                'total_tweets': 0,
                'tweets_with_contracts': 0,
                'unique_contracts': 0,
                'contracts_analysis': {}
            }

async def test_profile_parser():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π"""
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π Twitter")
    
    test_usernames = ['LaunchOnPump', 'elonmusk', 'pumpdotfun']
    
    async with TwitterProfileParser() as parser:
        profiles = await parser.get_multiple_profiles(test_usernames)
        
        for username, profile in profiles.items():
            if profile:
                logger.info(f"üìä –ü—Ä–æ—Ñ–∏–ª—å @{username}:")
                logger.info(f"  ‚Ä¢ –û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è: {profile.get('display_name', 'N/A')}")
                logger.info(f"  ‚Ä¢ –ü–æ–¥–ø–∏—Å—á–∏–∫–∏: {profile.get('followers_count', 0):,}")
                logger.info(f"  ‚Ä¢ –¢–≤–∏—Ç—ã: {profile.get('tweets_count', 0):,}")
                logger.info(f"  ‚Ä¢ –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω: {profile.get('is_verified', False)}")
                logger.info(f"  ‚Ä¢ –î–∞—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {profile.get('join_date', 'N/A')}")
            else:
                logger.info(f"‚ùå –ü—Ä–æ—Ñ–∏–ª—å @{username}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")

async def test_profile_with_replies():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π —Å replies –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π Twitter —Å /with_replies –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π")
    
    test_username = 'Tsomisol'  # –ü—Ä–∏–º–µ—Ä –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    
    async with TwitterProfileParser() as parser:
        profile_data, all_tweets, tweets_with_contracts = await parser.get_profile_with_replies_multi_page(test_username, max_pages=3)
        
        if profile_data:
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è @{test_username}:")
            logger.info(f"  ‚Ä¢ –û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è: {profile_data.get('display_name', 'N/A')}")
            logger.info(f"  ‚Ä¢ –ü–æ–¥–ø–∏—Å—á–∏–∫–∏: {profile_data.get('followers_count', 0):,}")
            logger.info(f"  ‚Ä¢ –¢–≤–∏—Ç—ã –Ω–∞ –ø—Ä–æ—Ñ–∏–ª–µ: {profile_data.get('tweets_count', 0):,}")
            logger.info(f"  ‚Ä¢ –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω: {profile_data.get('is_verified', False)}")
            logger.info(f"  ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {profile_data.get('pages_loaded', 0)}")
            logger.info(f"  ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ç–≤–∏—Ç–æ–≤: {profile_data.get('total_loaded_tweets', 0)}")
            logger.info(f"  ‚Ä¢ –¢–≤–∏—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏: {profile_data.get('tweets_with_contracts', 0)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–≤–∏—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏
            if tweets_with_contracts:
                logger.info(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã —Ç–≤–∏—Ç–æ–≤ —Å –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞–º–∏:")
                for i, tweet in enumerate(tweets_with_contracts[:3], 1):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ tweet —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
                    if isinstance(tweet, dict):
                        logger.info(f"  {i}. [{tweet.get('date', 'N/A')}] {tweet.get('text', '')[:100]}...")
                        logger.info(f"     üîó –ö–æ–Ω—Ç—Ä–∞–∫—Ç—ã: {', '.join(tweet.get('contracts', [])[:2])}")
                    else:
                        logger.info(f"  {i}. –¢–≤–∏—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (—Ç–∏–ø: {type(tweet)})")
        else:
            logger.info(f"‚ùå –ü—Ä–æ—Ñ–∏–ª—å @{test_username}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å")

if __name__ == "__main__":
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    
    from logger_config import setup_logging
    from dotenv import load_dotenv
    
    load_dotenv()
    setup_logging()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–µ—Å—Ç–∞
    if len(sys.argv) > 1 and sys.argv[1] == "replies":
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ —Å /with_replies –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π")
        asyncio.run(test_profile_with_replies())
    else:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π")
        asyncio.run(test_profile_parser()) 